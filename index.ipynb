{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:00.095166Z",
     "start_time": "2024-06-14T11:28:00.091958Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all bs4 langchain-nomic\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:01.711049Z",
     "start_time": "2024-06-14T11:28:00.096187Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:01.724846Z",
     "start_time": "2024-06-14T11:28:01.712224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-settings\n",
      "  Downloading pydantic_settings-2.3.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/mahbub/anaconda3/envs/jupyter_env/lib/python3.11/site-packages (from pydantic-settings) (2.7.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/mahbub/anaconda3/envs/jupyter_env/lib/python3.11/site-packages (from pydantic-settings) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/mahbub/anaconda3/envs/jupyter_env/lib/python3.11/site-packages (from pydantic>=2.7.0->pydantic-settings) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/mahbub/anaconda3/envs/jupyter_env/lib/python3.11/site-packages (from pydantic>=2.7.0->pydantic-settings) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/mahbub/anaconda3/envs/jupyter_env/lib/python3.11/site-packages (from pydantic>=2.7.0->pydantic-settings) (4.12.2)\n",
      "Downloading pydantic_settings-2.3.3-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pydantic-settings\n",
      "Successfully installed pydantic-settings-2.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "langchain_api_key = os.environ.get(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:21.813402Z",
     "start_time": "2024-06-14T11:28:15.490313Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:21.824537Z",
     "start_time": "2024-06-14T11:28:21.817230Z"
    }
   },
   "outputs": [],
   "source": [
    "run_local = \"Yes\"\n",
    "local_llm = \"mistral:instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:23.014493Z",
     "start_time": "2024-06-14T11:28:21.826008Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=100\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Embed and index\n",
    "if run_local == \"Yes\":\n",
    "    embedding = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs=gpt4all_kwargs)\n",
    "else:\n",
    "    embedding = MistralAIEmbeddings(mistral_api_key=mistral_api_key)\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahbub/anaconda3/envs/jupyter_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# LLM\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "else:\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
    "    )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To generate the most salient high-level questions given a set of observations, I will first list the 100 most recent observations and then ask three questions based on those observations. Here are the observations:\n",
      "\n",
      "1. The weather is sunny today.\n",
      "2. There is a new email from work.\n",
      "3. The stock market has been volatile lately.\n",
      "4. A friend invited me to dinner tonight.\n",
      "5. I have an important meeting tomorrow morning.\n",
      "6. My favorite sports team won their game last night.\n",
      "7. I need to buy groceries this weekend.\n",
      "8. There is a new software update for my phone.\n",
      "9. I received a notification about a sale at my favorite store.\n",
      "10. I have an appointment with the dentist next week.\n",
      "11. My car needs an oil change soon.\n",
      "12. The news reported a natural disaster in another country.\n",
      "13. I need to finish a project for work by Friday.\n",
      "14. A new movie is coming out this weekend that I want to see.\n",
      "15. I have a doctor's appointment next month.\n",
      "16. My favorite TV show has a new episode airing tonight.\n",
      "17. There is a sale on clothes at my favorite store.\n",
      "18. I need to renew my driver's license soon.\n",
      "19. The weather forecast for tomorrow is rainy.\n",
      "20. I have a deadline for a personal project next week.\n",
      "21. My friend asked if I wanted to go hiking this weekend.\n",
      "22. There is a new book by my favorite author coming out soon.\n",
      "23. I need to clean my house this weekend.\n",
      "24. The grocery store is having a sale on produce this week.\n",
      "25. I have an important call with a client tomorrow afternoon.\n",
      "26. My car's insurance is up for renewal next month.\n",
      "27. There is a concert in town that I want to go to this weekend.\n",
      "28. I need to buy a birthday gift for a friend next week.\n",
      "29. The weather forecast for the weekend is nice.\n",
      "30. My favorite sports team has a game this weekend.\n",
      "31. I have an important meeting with my boss tomorrow.\n",
      "32. There is a new video game coming out that I want to buy.\n",
      "33. I need to make dinner plans for tonight.\n",
      "34. The news reported a political event in another country.\n",
      "35. My favorite restaurant has a new menu item that I want to try.\n",
      "36. I have an important deadline for a work project next week.\n",
      "37. There is a sale on electronics at my favorite store.\n",
      "38. I need to schedule a haircut appointment soon.\n",
      "39. The weather forecast for the upcoming week is mixed.\n",
      "40. My friend asked if I wanted to go to a concert with them this weekend.\n",
      "41. A new season of my favorite TV show is starting soon.\n",
      "42. I have an important deadline for a personal project next month.\n",
      "43. There is a new episode of my favorite podcast coming out today.\n",
      "44. My car needs new tires soon.\n",
      "45. I need to buy a gift for a holiday next month.\n",
      "46. The weather forecast for the upcoming weekend is nice.\n",
      "47. My favorite sports team has a game this week.\n",
      "48. There is a new book by a popular author coming out soon.\n",
      "49. I have an important call with a friend tomorrow afternoon.\n",
      "50. My favorite store is having a sale on shoes this week.\n",
      "51. I need to make travel plans for a vacation next year.\n",
      "52. The news reported a local event in my city.\n",
      "53. There is a new movie coming out that I want to see with my family.\n",
      "54. My favorite sports team has a game this week.\n",
      "55. I have an important meeting with a colleague tomorrow morning.\n",
      "56. There is a new video game coming out that I want to pre-order.\n",
      "57. I need to make dinner plans for tonight with my family.\n",
      "58. The news reported a natural disaster in another part of the country.\n",
      "59. My favorite restaurant has a new dish that I want to try.\n",
      "60. I have an important deadline for a work project next week.\n",
      "61. There is a sale on furniture at my favorite store.\n",
      "62. I need to schedule a dental cleaning appointment soon.\n",
      "63. The weather forecast for the upcoming week is rainy.\n",
      "64. My friend asked if I wanted to go to a concert with them this week.\n",
      "65. A new season of my favorite TV show is starting next month.\n",
      "66. I have an important deadline for a personal project next year.\n",
      "67. There is a new episode of my favorite podcast coming out tomorrow.\n",
      "68. My car needs a tune-up soon.\n",
      "69. I need to buy a gift for a holiday next week.\n",
      "70. The weather forecast for the upcoming weekend is nice.\n",
      "71. My favorite sports team has a game this month.\n",
      "72. There is a new book by a popular author coming out soon.\n",
      "73. I have an important call with a client tomorrow morning.\n",
      "74. My favorite store is having a sale on appliances this week.\n",
      "75. I need to make dinner plans for tonight with my friends.\n",
      "76. The news reported a political event in another country.\n",
      "77. My favorite restaurant has a new menu item that I want to try with my family.\n",
      "78. I have an important deadline for a work project next month.\n",
      "79. There is a sale on electronics at my favorite store.\n",
      "80. I need to schedule a haircut appointment soon with my friends.\n",
      "81. The weather forecast for the upcoming week is mixed.\n",
      "82. My friend asked if I wanted to go hiking with them this week.\n",
      "83. A new season of my favorite TV show is starting next year.\n",
      "84. I have an important deadline for a personal project next year.\n",
      "85. There is a new episode of my favorite podcast coming out tomorrow with my friends.\n",
      "86. My car needs new brakes soon.\n",
      "87. I need to buy a gift for a holiday next month.\n",
      "88. The weather forecast for the upcoming weekend is nice.\n",
      "89. My favorite sports team has a game this month.\n",
      "90. There is a new book by a popular author coming out soon with my family.\n",
      "91. I have an important call with a friend tomorrow morning with my friends.\n",
      "92. My favorite store is having a sale on clothing this week.\n",
      "93. I need to make dinner plans for tonight with my coworkers.\n",
      "94. The news reported a local event in another city.\n",
      "95. There is a new movie coming out that I want to see with my coworkers.\n",
      "96. My favorite restaurant has a new dish that I want to try with my coworkers.\n",
      "97. I have an important deadline for a work project next year with my coworkers.\n",
      "98. There is a sale on furniture at my favorite store with my friends.\n",
      "99. I need to schedule a dental cleaning appointment soon with my coworkers.\n",
      "100. The weather forecast for the upcoming week is rainy with my family.\n",
      "\n",
      "Questions:\n",
      "1. What are some events or activities mentioned in this list?\n",
      "Answer: Some events or activities mentioned in this list include concerts, movies, video games, podcasts, TV shows, political events, natural disasters, local events, travel, vacations, furniture sales, clothing sales, appliance sales, electronics sales, haircuts, dental cleanings, hiking, dining out, and sports games.\n",
      "\n",
      "2. What are some personal or professional deadlines mentioned in this list?\n",
      "Answer: Some personal or professional deadlines mentioned in this list include important calls, meetings, holidays, birthdays, and project deadlines for work and personal projects.\n",
      "\n",
      "3. What are some transportation-related items mentioned in this list?\n",
      "Answer: Some transportation-related items mentioned in this list include cars, tires, brakes, insurance, and tune-ups.\n",
      "\n",
      "4. What are some shopping-related items mentioned in this list?\n",
      "Answer: Some shopping-related items mentioned in this list include furniture, appliances, electronics, clothing, shoes, gifts, and sales at various stores.\n",
      "\n",
      "5. What are some social interactions mentioned in this list?\n",
      "Answer: Some social interactions mentioned in this list include dinner plans, going out with friends, family, coworkers, and invitations to concerts, hiking, and other activities.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "else:\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
    "    )\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" What is the function of an agent's memory in a given context?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "else:\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the initial and formulate an improved question. \\n\n",
    "     Here is the initial question: \\n\\n {question}. Improved question with no preamble: \\n \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:ÃŸ\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "(' In an LLM (Large Language Model) powered autonomous agent system, the model '\n",
      " \"functions as the agent's brain. The key components of this system include \"\n",
      " 'Planning, Memory, and Tool use.\\n'\n",
      " '\\n'\n",
      " '1. Planning involves breaking down large tasks into smaller, manageable '\n",
      " 'subgoals for efficient handling of complex tasks. It also includes '\n",
      " 'self-criticism, learning from mistakes, and refining actions to improve '\n",
      " 'results.\\n'\n",
      " '\\n'\n",
      " \"2. Memory plays a crucial role in the agent's functioning. Short-term memory \"\n",
      " 'utilizes in-context learning (Prompt Engineering) to learn, while long-term '\n",
      " 'memory allows the agent to retain and recall information over extended '\n",
      " 'periods by leveraging an external vector store and fast retrieval.\\n'\n",
      " '\\n'\n",
      " '3. Tool use enables the agent to call external APIs for additional '\n",
      " 'information that is missing from the model weights. This can include current '\n",
      " 'information, code execution capability, access to proprietary information '\n",
      " 'sources, and more.\\n'\n",
      " '\\n'\n",
      " 'The article also mentions types of memory in human brains, such as Sensory '\n",
      " 'Memory, Short-Term Memory (STM), Long-Term Memory (LTM), '\n",
      " 'Explicit/Declarative Memory, and Implicit/Procedural Memory. These memories '\n",
      " 'store information for varying durations and serve different purposes.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search_node':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "(' Sam Altman and Elon Musk are both entrepreneurs and have worked together in '\n",
      " \"various capacities. Altman was the President of Y Combinator, where Musk's \"\n",
      " 'companies SpaceX and Tesla were incubated. They also served on the board of '\n",
      " 'directors for OpenAI, a research organization focused on artificial '\n",
      " 'intelligence. However, their relationship beyond these professional '\n",
      " 'collaborations is not extensively documented in the provided context.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"What is the relationship between Sam ALtman and Elon Musk?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
